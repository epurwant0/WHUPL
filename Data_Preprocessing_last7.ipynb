{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abc97c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67904155",
   "metadata": {},
   "outputs": [],
   "source": [
    "xls = pd.ExcelFile('Resources/Car_Data.xlsx')\n",
    "#df1 = pd.read_excel(xls, 'Volkswagon')\n",
    "#df2 = pd.read_excel(xls, 'Toyota')\n",
    "#df3 = pd.read_excel(xls, 'Porsche')\n",
    "#df4 = pd.read_excel(xls, 'Mercedes')\n",
    "#df5 = pd.read_excel(xls, 'Maserati')\n",
    "#df6 = pd.read_excel(xls, 'Lincoln')\n",
    "#df7 = pd.read_excel(xls, 'Lexus')\n",
    "df8 = pd.read_excel(xls, 'Kia')\n",
    "df9 = pd.read_excel(xls, 'Landrover')\n",
    "df10 = pd.read_excel(xls, 'Honda')\n",
    "df11 = pd.read_excel(xls, 'Hyundai')\n",
    "df12 = pd.read_excel(xls, 'Acura')\n",
    "df13 = pd.read_excel(xls, 'Audi')\n",
    "df14 = pd.read_excel(xls, 'BMW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6f00feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    new_df = df.dropna(how='any')\n",
    "    new_df['formatted_sold_date'] = pd.to_datetime(new_df['sold_date'], format='%Y-%m-%d-%H.%M.%S.%f')\n",
    "    cc_df = new_df.drop(columns=['sold_date'])\n",
    "    cc_df = cc_df.drop(cc_df[(cc_df['condition_grade'] == 'IO') | (cc_df['condition_grade'] == 'I0')].index)\n",
    "    cc_df = cc_df.drop(cc_df[(cc_df['color'] == 'UNKNOWN')].index)\n",
    "    cc_df['condition_grade'] = cc_df['condition_grade'].replace({'A': 'AV', 'C': 'CL'})\n",
    "    cc_df['condition_grade'] = cc_df['condition_grade'].replace(\n",
    "    {'SL': 0, 'RG': 20, 'PR': 10, 'EC':50, 'CL':40, 'AV':30})\n",
    "\n",
    "    return cc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6c34e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-7d1c117e109e>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['formatted_sold_date'] = pd.to_datetime(new_df['sold_date'], format='%Y-%m-%d-%H.%M.%S.%f')\n",
      "<ipython-input-3-7d1c117e109e>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['formatted_sold_date'] = pd.to_datetime(new_df['sold_date'], format='%Y-%m-%d-%H.%M.%S.%f')\n",
      "<ipython-input-3-7d1c117e109e>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['formatted_sold_date'] = pd.to_datetime(new_df['sold_date'], format='%Y-%m-%d-%H.%M.%S.%f')\n",
      "<ipython-input-3-7d1c117e109e>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['formatted_sold_date'] = pd.to_datetime(new_df['sold_date'], format='%Y-%m-%d-%H.%M.%S.%f')\n",
      "<ipython-input-3-7d1c117e109e>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['formatted_sold_date'] = pd.to_datetime(new_df['sold_date'], format='%Y-%m-%d-%H.%M.%S.%f')\n",
      "<ipython-input-3-7d1c117e109e>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['formatted_sold_date'] = pd.to_datetime(new_df['sold_date'], format='%Y-%m-%d-%H.%M.%S.%f')\n",
      "<ipython-input-3-7d1c117e109e>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['formatted_sold_date'] = pd.to_datetime(new_df['sold_date'], format='%Y-%m-%d-%H.%M.%S.%f')\n"
     ]
    }
   ],
   "source": [
    "clean_df8 = clean_data(df8)\n",
    "clean_df9 = clean_data(df9)\n",
    "clean_df10 = clean_data(df10)\n",
    "clean_df11 = clean_data(df11)\n",
    "clean_df12 = clean_data(df12)\n",
    "clean_df13 = clean_data(df13)\n",
    "clean_df14 = clean_data(df14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a8a1cc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_df = df8.dropna(how='any')\n",
    "#clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "663d8d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff300ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate entries: 0\n",
      "Duplicate entries: 1\n",
      "Duplicate entries: 2\n",
      "Duplicate entries: 1\n",
      "Duplicate entries: 0\n",
      "Duplicate entries: 1\n",
      "Duplicate entries: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Duplicate entries: {clean_df8.duplicated().sum()}\")\n",
    "print(f\"Duplicate entries: {clean_df9.duplicated().sum()}\")\n",
    "print(f\"Duplicate entries: {clean_df10.duplicated().sum()}\")\n",
    "print(f\"Duplicate entries: {clean_df11.duplicated().sum()}\")\n",
    "print(f\"Duplicate entries: {clean_df12.duplicated().sum()}\")\n",
    "print(f\"Duplicate entries: {clean_df13.duplicated().sum()}\")\n",
    "print(f\"Duplicate entries: {clean_df14.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6123fb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate entries: 0\n"
     ]
    }
   ],
   "source": [
    "no_dups_df9 = clean_df9.drop_duplicates()\n",
    "print(f\"Duplicate entries: {no_dups_df9.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50940ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate entries: 0\n"
     ]
    }
   ],
   "source": [
    "no_dups_df10 = clean_df10.drop_duplicates()\n",
    "print(f\"Duplicate entries: {no_dups_df10.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6141293d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate entries: 0\n"
     ]
    }
   ],
   "source": [
    "no_dups_df11 = clean_df11.drop_duplicates()\n",
    "print(f\"Duplicate entries: {no_dups_df11.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06c1fff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate entries: 0\n"
     ]
    }
   ],
   "source": [
    "no_dups_df13 = clean_df13.drop_duplicates()\n",
    "print(f\"Duplicate entries: {no_dups_df13.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed8dc504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out csv's to output folder\n",
    "clean_df8.to_csv('Output/KIA.csv', index = False, header = True)\n",
    "clean_df9.to_csv('Output/Land_Rover.csv', index = False, header = True)\n",
    "clean_df10.to_csv('Output/Honda.csv', index = False, header = True)\n",
    "clean_df11.to_csv('Output/Hyundai.csv', index = False, header = True)\n",
    "clean_df12.to_csv('Output/Acura.csv', index = False, header = True)\n",
    "clean_df13.to_csv('Output/Audi.csv', index = False, header = True)\n",
    "clean_df14.to_csv('Output/BMW.csv', index = False, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "33d6f201",
   "metadata": {},
   "outputs": [],
   "source": [
    "df8_dummies = pd.get_dummies(clean_df8)\n",
    "df9_dummies = pd.get_dummies(clean_df9)\n",
    "df10_dummies = pd.get_dummies(clean_df10)\n",
    "df11_dummies = pd.get_dummies(clean_df11)\n",
    "df12_dummies = pd.get_dummies(clean_df12)\n",
    "df13_dummies = pd.get_dummies(clean_df13)\n",
    "df14_dummies = pd.get_dummies(clean_df14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bea4b81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the label to create the X data\n",
    "X_8 = df8_dummies.drop('sale_price', axis=1)\n",
    "X_9 = df9_dummies.drop('sale_price', axis=1)\n",
    "X_10 = df10_dummies.drop('sale_price', axis=1)\n",
    "X_11 = df11_dummies.drop('sale_price', axis=1)\n",
    "X_12 = df12_dummies.drop('sale_price', axis=1)\n",
    "X_13 = df13_dummies.drop('sale_price', axis=1)\n",
    "X_14 = df14_dummies.drop('sale_price', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c8d4b47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_8 = df8_dummies[\"sale_price\"]\n",
    "y_9 = df9_dummies[\"sale_price\"]\n",
    "y_10 = df10_dummies[\"sale_price\"]\n",
    "y_11 = df11_dummies[\"sale_price\"]\n",
    "y_12 = df12_dummies[\"sale_price\"]\n",
    "y_13 = df13_dummies[\"sale_price\"]\n",
    "y_14 = df14_dummies[\"sale_price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "72d51bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train8, X_test8, y_train8, y_test8 = train_test_split(X_8, y_8, random_state=1)\n",
    "X_train9, X_test9, y_train9, y_test9 = train_test_split(X_9, y_9, random_state=1)\n",
    "X_train10, X_test10, y_train10, y_test10 = train_test_split(X_11, y_11, random_state=1)\n",
    "X_train11, X_test11, y_train11, y_test11 = train_test_split(X_12, y_12, random_state=1)\n",
    "X_train12, X_test12, y_train12, y_test12 = train_test_split(X_13, y_13, random_state=1)\n",
    "X_train13, X_test13, y_train13, y_test13 = train_test_split(X_14, y_14, random_state=1)\n",
    "X_train14, X_test14, y_train14, y_test14 = train_test_split(X_15, y_15, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "291484c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The DTypes <class 'numpy.dtype[datetime64]'> and <class 'numpy.dtype[int64]'> do not have a common DType. For example they cannot be stored in a single array unless the dtype is `object`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-f402c05a620d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Scaling the X data by using StandardScaler()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mscaler1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX_train_scaled8\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mX_train_scaled8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    728\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 730\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    731\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    732\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    764\u001b[0m         \"\"\"\n\u001b[0;32m    765\u001b[0m         \u001b[0mfirst_call\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"n_samples_seen_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 766\u001b[1;33m         X = self._validate_data(X, accept_sparse=('csr', 'csc'),\n\u001b[0m\u001b[0;32m    767\u001b[0m                                 \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    768\u001b[0m                                 force_all_finite='allow-nan', reset=first_call)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    419\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'no_validation'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdtypes_orig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 540\u001b[1;33m             \u001b[0mdtype_orig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdtypes_orig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    541\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype_numeric\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mresult_type\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: The DTypes <class 'numpy.dtype[datetime64]'> and <class 'numpy.dtype[int64]'> do not have a common DType. For example they cannot be stored in a single array unless the dtype is `object`."
     ]
    }
   ],
   "source": [
    "# Scaling the X data by using StandardScaler()\n",
    "scaler1 = StandardScaler().fit(X_train8)\n",
    "X_train_scaled8 = scaler.transform(X_train8)\n",
    "X_train_scaled8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e1587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df_shopping[['Age', 'Annual Income', 'Spending Score (1-100)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b87959",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
